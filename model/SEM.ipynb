{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a81a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: John PouguÃ©-Biyong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f621165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "import pkg_resources\n",
    "import gc\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import networkx as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, roc_curve, roc_auc_score\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import LongTensor as LT\n",
    "from torch import FloatTensor as FT\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = t.device(\"cuda:6\" if t.cuda.is_available() else \"cpu\")\n",
    "cuda = t.cuda.is_available()\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,\n",
    "                            sampling_strategy: dict = None, num_walks_key: str = None, \n",
    "                            walk_length_key: str = None, neighbors_key: str = None, \n",
    "                            probabilities_key: str = None, first_travel_key: str = None,\n",
    "                            quiet: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Generates the random walks which will be used as the skip-gram input.\n",
    "    Returns list of walks. Each walk is a list of nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    walks = list()\n",
    "\n",
    "    if not quiet:\n",
    "        pbar = tqdm(total=num_walks, desc='Generating walks (CPU: {})'.format(cpu_num))\n",
    "\n",
    "    for n_walk in range(num_walks):\n",
    "\n",
    "        # Update progress bar\n",
    "        if not quiet:\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Shuffle the nodes\n",
    "        shuffled_nodes = [key for key in d_graph if len(d_graph[key][neighbors_key]) > 0]\n",
    "        rd.shuffle(shuffled_nodes)\n",
    "\n",
    "        # Start a random walk from every node\n",
    "        for source in shuffled_nodes:\n",
    "          \n",
    "            # Skip nodes with specific num_walks\n",
    "            if source in sampling_strategy and \\\n",
    "                    num_walks_key in sampling_strategy[source] and \\\n",
    "                    sampling_strategy[source][num_walks_key] <= n_walk:\n",
    "                continue\n",
    "\n",
    "            # Start walk\n",
    "            walk = [source]\n",
    "\n",
    "            # Calculate walk length\n",
    "            if source in sampling_strategy:\n",
    "                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)\n",
    "            else:\n",
    "                walk_length = global_walk_length\n",
    "\n",
    "            # Perform walk\n",
    "            while len(walk) < walk_length:\n",
    "                walk_options = d_graph[walk[-1]].get(neighbors_key, None)\n",
    "\n",
    "                # Skip dead end nodes\n",
    "                if not walk_options:\n",
    "                    break\n",
    "\n",
    "                if len(walk) == 1:  # For the first step\n",
    "                    probabilities = d_graph[walk[-1]][first_travel_key]\n",
    "                    walk_to = np.random.choice(walk_options, size=1, p=probabilities)[0]\n",
    "                else:\n",
    "                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]\n",
    "                    walk_to = np.random.choice(walk_options, size=1, p=probabilities)[0]\n",
    "\n",
    "                walk.append(walk_to)\n",
    "\n",
    "            walk = list(map(str, walk))  # Convert all to strings\n",
    "\n",
    "            walks.append(walk)\n",
    "\n",
    "    if not quiet:\n",
    "        pbar.close()\n",
    "    return walks\n",
    "\n",
    "\n",
    "class WalksGenerator:\n",
    "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
    "    PROBABILITIES_KEY = 'probabilities'\n",
    "    NEIGHBORS_KEY = 'neighbors'\n",
    "    WEIGHT_KEY = 'weight'\n",
    "    NUM_WALKS_KEY = 'num_walks'\n",
    "    WALK_LENGTH_KEY = 'walk_length'\n",
    "    P_KEY = 'p'\n",
    "    Q_KEY = 'q'\n",
    "\n",
    "    def __init__(self, graph: nx.Graph, dimensions: int = 64, \n",
    "                 walk_length: int = 80, num_walks: int = 10, p: float = 1,\n",
    "                 q: float = 1, weight_key: str = 'weight', workers: int = 1, \n",
    "                 sampling_strategy: dict = None,\n",
    "                 quiet: bool = False, temp_folder: str = None, seed: int = None):\n",
    "        \"\"\"\n",
    "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
    "        :param graph: Input graph\n",
    "        :param dimensions: Embedding dimensions (default: 64)\n",
    "        :param walk_length: Number of nodes in each walk (default: 80)\n",
    "        :param num_walks: Number of walks per node (default: 10)\n",
    "        :param p: Return hyper parameter (default: 1)\n",
    "        :param q: Inout parameter (default: 1)\n",
    "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
    "        :param workers: Number of workers for parallel execution (default: 1)\n",
    "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
    "        :param seed: Seed for the random number generator.\n",
    "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
    "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = graph\n",
    "        self.dimensions = dimensions\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.weight_key = weight_key\n",
    "        self.workers = workers\n",
    "        self.quiet = quiet\n",
    "        self.d_graph = defaultdict(dict)\n",
    "\n",
    "        if sampling_strategy is None:\n",
    "            self.sampling_strategy = {}\n",
    "        else:\n",
    "            self.sampling_strategy = sampling_strategy\n",
    "\n",
    "        self.temp_folder, self.require = None, None\n",
    "        if temp_folder:\n",
    "            if not os.path.isdir(temp_folder):\n",
    "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\" \\\n",
    "                                         .format(temp_folder))\n",
    "\n",
    "            self.temp_folder = temp_folder\n",
    "            self.require = \"sharedmem\"\n",
    "\n",
    "        if seed is not None:\n",
    "            rd.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self._precompute_probabilities()\n",
    "\n",
    "    def _precompute_probabilities(self):\n",
    "        \"\"\"\n",
    "        Precomputes transition probabilities for each node.\n",
    "        \"\"\"\n",
    "\n",
    "        d_graph = self.d_graph\n",
    "\n",
    "        nodes_generator = self.graph.nodes() if self.quiet \\\n",
    "            else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
    "\n",
    "        for source in nodes_generator:\n",
    "\n",
    "            # Init probabilities dict for first travel\n",
    "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
    "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "            for current_node in self.graph.neighbors(source):\n",
    "\n",
    "                # Init probabilities dict\n",
    "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
    "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "                unnormalized_weights = list()\n",
    "                d_neighbors = list()\n",
    "\n",
    "                # Calculate unnormalized weights\n",
    "                for destination in self.graph.neighbors(current_node):\n",
    "\n",
    "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
    "                                                                 self.p) \\\n",
    "                    if current_node in self.sampling_strategy else self.p\n",
    "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
    "                                                                 self.q) \\\n",
    "                    if current_node in self.sampling_strategy else self.q\n",
    "\n",
    "                    if destination == source:  # Backwards probability\n",
    "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / p\n",
    "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
    "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
    "                    else:\n",
    "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / q\n",
    "\n",
    "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
    "                    unnormalized_weights.append(ss_weight)\n",
    "                    d_neighbors.append(destination)\n",
    "\n",
    "                # Normalize\n",
    "                unnormalized_weights = np.array(unnormalized_weights)\n",
    "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
    "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
    "\n",
    "            # Calculate first_travel weights for source\n",
    "            first_travel_weights = []\n",
    "\n",
    "            for destination in self.graph.neighbors(source):\n",
    "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
    "\n",
    "            first_travel_weights = np.array(first_travel_weights)\n",
    "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
    "\n",
    "            # Save neighbors\n",
    "            d_graph[source][self.NEIGHBORS_KEY] = list(self.graph.neighbors(source))\n",
    "\n",
    "    def generate_walks(self, workers) -> list:\n",
    "        \"\"\"\n",
    "        Generates the random walks which will be used as the skip-gram input.\n",
    "        Returns list of walks. Each walk is a list of nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        # Split num_walks for each worker\n",
    "        num_walks_lists = np.array_split(range(self.num_walks), workers)\n",
    "\n",
    "        walk_results = Parallel(n_jobs=workers, temp_folder=None, require=None)(\n",
    "            delayed(parallel_generate_walks)(self.d_graph,\n",
    "                                             self.walk_length,\n",
    "                                             len(num_walks),\n",
    "                                             idx,\n",
    "                                             self.sampling_strategy,\n",
    "                                             self.NUM_WALKS_KEY,\n",
    "                                             self.WALK_LENGTH_KEY,\n",
    "                                             self.NEIGHBORS_KEY,\n",
    "                                             self.PROBABILITIES_KEY,\n",
    "                                             self.FIRST_TRAVEL_KEY,\n",
    "                                             self.quiet) for\n",
    "            idx, num_walks\n",
    "            in enumerate(num_walks_lists, 1))\n",
    "\n",
    "        self.walks = flatten(walk_results)\n",
    "\n",
    "class Walks(object):\n",
    "  \n",
    "  def __init__(self, abs_digraph, \n",
    "               walk_len=15, num_walks=10, p=.5, q=1.5, \n",
    "               workers=1, seed=2021):\n",
    "      \"\"\" \n",
    "      Generate walks.\n",
    "      Input:\n",
    "          abs_digraph nx.DiGraph: absolute signed graph\n",
    "          walk_len int: length of the walks\n",
    "          num_walks int: #walks per node\n",
    "          workers int: #workers for parallelization\n",
    "          p float: return hyperparameter\n",
    "          q float: inout hyperparameter\n",
    "          seed int: random seed state\n",
    "      \"\"\"\n",
    "      self.walk_generator = WalksGenerator(abs_digraph,\n",
    "                                           dimensions=64,\n",
    "                                           walk_length=walk_len,\n",
    "                                           num_walks=num_walks,\n",
    "                                           weight_key='weight',\n",
    "                                           quiet=False,\n",
    "                                           workers=workers, \n",
    "                                           p=p, \n",
    "                                           q=q, \n",
    "                                           seed=seed)\n",
    "  \n",
    "  def generate(self, workers=1):\n",
    "    \"\"\" Generate walks. \"\"\"\n",
    "    self.walk_generator.generate_walks(workers)\n",
    "    self.walks = self.walk_generator.walks \n",
    "    del self.walk_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts(walk, i, window, adj_mat, unk):\n",
    "        \"\"\" \n",
    "        Creates contexts with multi-step edge sign estimation.\n",
    "        Input:\n",
    "          walk list\n",
    "          i int: current node\n",
    "        Returns:\n",
    "          inode int: current node\n",
    "          left + right list: contexts\n",
    "        \"\"\"\n",
    "        inode = int(walk[i])\n",
    "        left_sign, right_sign = 1, 1\n",
    "        left, right = [], []\n",
    "        last_visited_left, last_visited_right = inode, inode\n",
    "        count = 1\n",
    "        while count < window + 1:\n",
    "            if i - count > -1:\n",
    "                context_node_left = int(walk[i - count])\n",
    "                current_sign_left = adj_mat[context_node_left, last_visited_left]\n",
    "                if current_sign_left not in [-1, 1]:\n",
    "                    print('!!')\n",
    "                    print(current_sign_left)\n",
    "                    print('---')\n",
    "                left_sign *= current_sign_left\n",
    "                if left_sign == 1:\n",
    "                    context_left = str(context_node_left) + '+'\n",
    "                elif left_sign == -1:\n",
    "                    context_left = str(context_node_left) + '-'\n",
    "                last_visited_left = context_node_left\n",
    "            else:\n",
    "                context_left = unk\n",
    "            \n",
    "            if i + count < len(walk):\n",
    "                context_node_right = int(walk[i + count])\n",
    "                current_sign_right = adj_mat[last_visited_right, context_node_right]\n",
    "                if current_sign_right not in [-1, 1]:\n",
    "                    print('!!')\n",
    "                    print(current_sign_right)\n",
    "                    print('---')\n",
    "                right_sign *= current_sign_right\n",
    "                if right_sign == 1:\n",
    "                    context_right = str(context_node_right) + '+'\n",
    "                elif right_sign == -1:\n",
    "                    context_right = str(context_node_right) + '-'\n",
    "                last_visited_right = context_node_right\n",
    "            else:\n",
    "                context_right = unk\n",
    "            \n",
    "            count += 1\n",
    "            left += [context_left]\n",
    "            right += [context_right]\n",
    "        left.reverse()\n",
    "        \n",
    "        return inode, left + right\n",
    "\n",
    "def parallel_create_contexts(walks, topic_idx, cpu_num, window, adj_mat, unk):\n",
    "        \"\"\" \n",
    "        Converts walks into (node, contexts) tuples.\n",
    "        Input:\n",
    "          walks list\n",
    "          topic_idx int\n",
    "          cpu_num int: # CPUs\n",
    "          window int: window size for the contexts\n",
    "          adj_mat sparse.csr_matrix: signed adjacency matrix\n",
    "          unk str: padding token\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        pbar = tqdm(total=len(walks), desc='Generating contexts (CPU: {})'.format(cpu_num))\n",
    "        for walk in walks:\n",
    "            pbar.update(1)\n",
    "            for i in range(len(walk)):\n",
    "                inode, ocontexts = create_contexts(walk, i, window, adj_mat, unk)\n",
    "                data.append((inode, ocontexts, topic_idx))\n",
    "        pbar.close()\n",
    "        return data\n",
    "\n",
    "class TrainingSamples(object):\n",
    "    \n",
    "    def __init__(self, window=5, adj_mat=None, \n",
    "                 idx2node={}, node2idx={}, idx2topic={}, topic2idx={},\n",
    "                 unk='<UNK>'):\n",
    "        \"\"\" \n",
    "        Input:\n",
    "          window int: window size for the contexts\n",
    "          adj_mat sparse.csr_matrix: signed adjacency matrix\n",
    "          idx2node dict: mapping index to node\n",
    "          node2idx dict: mapping node to index\n",
    "          unk str: padding token\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.unk = unk\n",
    "        self.adj_mat = adj_mat\n",
    "        self.node2idx = node2idx\n",
    "        node2idx[self.unk] = len(node2idx)\n",
    "        self.idx2node = idx2node\n",
    "        idx2node[len(idx2node)] = self.unk\n",
    "        self.node_vocab = set([node for node in self.node2idx])\n",
    "        self.topic2idx = topic2idx\n",
    "        self.idx2topic = idx2topic\n",
    "        self.topic_vocab = set([topic for topic in self.topic2idx])\n",
    "        self.data = None\n",
    "\n",
    "    def build_context_vocab(self):\n",
    "        step = 0\n",
    "        self.cc = {self.unk: 1}\n",
    "        print(\"computing context frequencies...\")\n",
    "        for (inode, ocontexts, topic_idx) in tqdm(self.data):\n",
    "            for context in ocontexts:\n",
    "                self.cc[context] = self.cc.get(context, 0) + 1\n",
    "        print(\"\")\n",
    "        print(\"building context vocab...\")\n",
    "        self.cc[self.unk] = 1\n",
    "        self.idx2context = [self.unk] + sorted(self.cc, key=self.cc.get, reverse=True)\n",
    "        self.context2idx = {self.idx2context[idx]: idx for idx, _ in enumerate(self.idx2context)}\n",
    "        self.context_vocab = set([context for context in self.context2idx])\n",
    "        data = []\n",
    "        size_data = len(self.data) \n",
    "        for _ in tqdm(range(size_data)):\n",
    "            inode, ocontexts, topic_idx = self.data.pop()\n",
    "            data.append((inode, [self.context2idx[ocontext] for ocontext in ocontexts], topic_idx))\n",
    "        self.data = data\n",
    "        print(\"building done\")\n",
    "      \n",
    "    def convert(self, walks, topic_idx, workers):\n",
    "        \n",
    "        print(\"converting corpus in parallel...\")\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        # Split num_walks for each worker\n",
    "        walks_lists = np.array_split(walks, workers)\n",
    "        \n",
    "        context_results = Parallel(n_jobs=workers, temp_folder=None, require=None)(\n",
    "            delayed(parallel_create_contexts)(wlks,\n",
    "                                              topic_idx,\n",
    "                                              idx, \n",
    "                                              self.window, \n",
    "                                              self.adj_mat, \n",
    "                                              self.unk) \\\n",
    "          for idx, wlks in enumerate(walks_lists, 1)\n",
    "        )\n",
    "        self.data = flatten(context_results)\n",
    "        print('conversion done')\n",
    "        \n",
    "    def info(self):\n",
    "        print('#nodes:', len(self.node_vocab))\n",
    "        print('#contexts:', len(self.context_vocab))\n",
    "        print('#training samples:',len(self.data))\n",
    "        print('An example:', self.data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "  \n",
    "    def __init__(self, edge_data, node2idx, idx2node, dataset, \n",
    "               path_walks, path_contexts, \n",
    "               dataset_type='hetero', topic2idx={'unk': 0}, idx2topic={0: 'unk'}):\n",
    "        self.dataset = dataset\n",
    "        self.dataset_type = dataset_type\n",
    "        self.edge_data = edge_data\n",
    "        self.node2idx = node2idx\n",
    "        self.idx2node = idx2node\n",
    "        self.topic2idx = topic2idx\n",
    "        self.idx2topic = idx2topic\n",
    "        self.path_walks = path_walks\n",
    "        self.path_contexts = path_contexts\n",
    "        self.size = len(self.node2idx)\n",
    "        print('#nodes:', self.size)\n",
    "        self.nb_topics = len(self.topic2idx)\n",
    "          \n",
    "    def build_training_data(self, p, q, nw, wl, wind, workers=16):\n",
    "        \"\"\" Builds networkX directed graphs from adjacency matrix.\"\"\"\n",
    "        print('building training samples...')\n",
    "\n",
    "        walks_available = False\n",
    "        contexts_available = False\n",
    "        my_walks_path = self.path_walks + '_p{p}_q{q}_nw{nw}_wl{wl}.json' \\\n",
    "        .format(tp=self.dataset_type, p=p, q=q, nw=nw, wl=wl)\n",
    "        my_file = Path(my_walks_path)\n",
    "        if my_file.is_file():\n",
    "            print('Walks exist.')\n",
    "            walks_available = True\n",
    "            with open(my_walks_path, 'r') as F:\n",
    "                self.all_walks = json.loads(F.read())\n",
    "            F.close()\n",
    "        else:\n",
    "            print('Walks dont exist.')\n",
    "        my_contexts_path = self.path_contexts + '_p{p}_q{q}_nw{nw}_wl{wl}_wind{wind}.json' \\\n",
    "        .format(tp=self.dataset_type, p=p, q=q, nw=nw, wl=wl, wind=wind)\n",
    "      \n",
    "        my_file = Path(my_contexts_path)\n",
    "        if my_file.is_file():\n",
    "            print('Contexts exist.')\n",
    "            contexts_available = True\n",
    "            with open(my_contexts_path, 'r') as F:\n",
    "                self.all_contexts = json.loads(F.read())\n",
    "            F.close()\n",
    "        else:\n",
    "            print('Contexts dont exist.')\n",
    "\n",
    "        if not (walks_available and contexts_available):\n",
    "            self.all_walks = []\n",
    "            self.all_contexts = []\n",
    "            for topic_idx in tqdm(self.idx2topic):\n",
    "                print('\\ntopic no {}:'.format(topic_idx), self.idx2topic[topic_idx])\n",
    "                dt = self.edge_data[self.edge_data.topic_idx == topic_idx]\n",
    "                row = dt.source_idx.values\n",
    "                col = dt.target_idx.values\n",
    "                data = dt.weight.values\n",
    "                adj_mat = sp.sparse.csr_matrix((data, (row, col)),\n",
    "                                                   shape=(self.size, self.size))\n",
    "                abs_adj_mat = \\\n",
    "                sp.sparse.csr_matrix(adj_mat.multiply(adj_mat))\n",
    "                digraph = nx.from_scipy_sparse_matrix(abs_adj_mat,\n",
    "                                                         create_using=nx.DiGraph)\n",
    "                start_time = time.time()\n",
    "                print('generating walks...')\n",
    "                walks_instance = Walks(digraph, p=p, q=q, num_walks=nw, walk_len=wl)\n",
    "                walks_instance.generate()\n",
    "                walks = walks_instance.walks\n",
    "                print('execution time:', time.time() - start_time)\n",
    "                print('#walks:', len(walks))\n",
    "                self.all_walks += [(walk, topic_idx) for walk in walks]\n",
    "                training_samples = TrainingSamples(window=wind,\n",
    "                                                 adj_mat=adj_mat,\n",
    "                                                 idx2node=deepcopy(self.idx2node),\n",
    "                                                 node2idx=deepcopy(self.node2idx),\n",
    "                                                 idx2topic=deepcopy(self.idx2topic),\n",
    "                                                 topic2idx=deepcopy(self.topic2idx),\n",
    "                                                 unk='<UNK>')\n",
    "                training_samples.convert(walks, topic_idx, workers=workers)\n",
    "                self.all_contexts += training_samples.data\n",
    "                del training_samples\n",
    "                del walks\n",
    "                del adj_mat\n",
    "                del abs_adj_mat\n",
    "                del digraph\n",
    "            with open(my_walks_path, 'w') as F:\n",
    "                F.write(json.dumps(self.all_walks))\n",
    "            F.close()\n",
    "            with open(my_contexts_path, 'w') as F:\n",
    "                F.write(json.dumps(self.all_contexts))\n",
    "            F.close()\n",
    "      \n",
    "        training_samples = TrainingSamples(idx2node=deepcopy(self.idx2node),\n",
    "                                         node2idx=deepcopy(self.node2idx),\n",
    "                                         idx2topic=deepcopy(self.idx2topic),\n",
    "                                         topic2idx=deepcopy(self.topic2idx),\n",
    "                                         unk='<UNK>')\n",
    "        training_samples.data = self.all_contexts\n",
    "        training_samples.build_context_vocab()\n",
    "        training_samples.info()\n",
    "        self.training_samples = training_samples\n",
    "        print('built training samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bundler(nn.Module):\n",
    "\n",
    "    def forward(self, data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward_i(self, data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward_o(self, data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Embeddings(Bundler):\n",
    "\n",
    "    def __init__(self, node_vocab_size=5000, topic_vocab_size=100, \n",
    "                 context_vocab_size=10001, embedding_size=64, op='addition'):\n",
    "        \"\"\" \n",
    "        Input:\n",
    "          node_vocab_size int: #nodes\n",
    "          context_vocab_size int: #contexts\n",
    "          embedding_size int\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.node_vocab_size = node_vocab_size\n",
    "        self.topic_vocab_size = topic_vocab_size\n",
    "        self.context_vocab_size = context_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.padding_idx = self.context_vocab_size - 1\n",
    "        self.op = op\n",
    "        \n",
    "        self.ivectors = nn.Embedding(self.node_vocab_size, self.embedding_size)\n",
    "        self.ivectors.weight = nn.Parameter(FT(self.node_vocab_size, self.embedding_size) \\\n",
    "                                            .uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size))\n",
    "        self.ivectors.weight.requires_grad = True\n",
    "        print(self.ivectors.weight.data.cpu().numpy().shape)\n",
    "        \n",
    "        self.itopics = nn.Embedding(self.topic_vocab_size, self.embedding_size)\n",
    "        if self.topic_vocab_size > 1:\n",
    "            self.multitopic = True\n",
    "            self.itopics.weight = nn.Parameter(FT(self.topic_vocab_size, self.embedding_size) \\\n",
    "                                                .uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size))\n",
    "            self.itopics.weight.requires_grad = True\n",
    "        elif self.topic_vocab_size == 1:\n",
    "            self.multitopic = False\n",
    "            self.itopics.weight = nn.Parameter(FT(self.topic_vocab_size, self.embedding_size) \\\n",
    "                                                .uniform_(0, 0))\n",
    "            self.itopics.weight.requires_grad = False\n",
    "        \n",
    "        self.ovectors = nn.Embedding(self.context_vocab_size, \n",
    "                                     self.embedding_size, \n",
    "                                     padding_idx=self.padding_idx)\n",
    "        self.ovectors.weight = nn.Parameter(t.cat([FT(self.context_vocab_size - 1, self.embedding_size) \\\n",
    "                                                   .uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size), \n",
    "                                                   t.zeros(1, self.embedding_size)]))\n",
    "        self.ovectors.weight.requires_grad = True\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.forward_i(data)\n",
    "\n",
    "    def forward_i(self, node, topic):\n",
    "        u = LT(node)\n",
    "        u = u.to(device)\n",
    "        if self.multitopic:\n",
    "            v = LT(topic)\n",
    "            v = v.to(device)\n",
    "            return self.__operation(self.ivectors(u), self.itopics(v))\n",
    "        else:\n",
    "            return self.ivectors(u)\n",
    "\n",
    "    def forward_o(self, context):\n",
    "        v = LT(context)\n",
    "        v = v.to(device)\n",
    "        return self.ovectors(v)\n",
    "    \n",
    "    def __operation(self, x, y):\n",
    "        if self.op == 'addition':\n",
    "            return x + y\n",
    "        elif self.op == 'hadamard':\n",
    "            return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6982549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    \"\"\" Skipgram with negative sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding, n_negs=20, weights=None):\n",
    "        \"\"\" \n",
    "        Input:\n",
    "          embedding: Embeddings object\n",
    "          n_negs int: #negative samples\n",
    "          weights np.array: context weights\n",
    "        \"\"\"\n",
    "        super(SGNS, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.context_vocab_size = self.embedding.context_vocab_size\n",
    "        self.n_negs = n_negs\n",
    "        self.weights = None\n",
    "        if weights is not None:\n",
    "            cf = np.power(weights, 0.75)\n",
    "            cf = cf / cf.sum()\n",
    "            self.weights = FT(cf)\n",
    "\n",
    "    def forward(self, inode, ocontexts, itopic):\n",
    "        batch_size = inode.size()[0]\n",
    "        context_size = ocontexts.size()[1]\n",
    "        if self.weights is not None:\n",
    "            ncontexts = t.multinomial(self.weights,\n",
    "                                      batch_size * context_size * self.n_negs,\n",
    "                                      replacement=True).view(batch_size, -1)\n",
    "        else:\n",
    "            ncontexts = FT(batch_size, context_size * self.n_negs) \\\n",
    "                        .uniform_(0, self.context_vocab_size - 1).long()\n",
    "        ivectors = self.embedding.forward_i(inode, itopic).unsqueeze(2)\n",
    "        ovectors = self.embedding.forward_o(ocontexts)\n",
    "        nvectors = self.embedding.forward_o(ncontexts).neg()\n",
    "        oloss = t.bmm(ovectors, ivectors).squeeze().sigmoid().log().mean(1)\n",
    "        nloss = t.bmm(nvectors, ivectors).squeeze().sigmoid().log() \\\n",
    "                .view(-1, context_size, self.n_negs).sum(2).mean(1)\n",
    "        return -(oloss + nloss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abed93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutedSubsampledCorpus:\n",
    "    \"\"\" For subsampling data if needed.\"\"\"\n",
    "    def __init__(self, data, idx2context, us=True, ratio=.8, cs=None):\n",
    "        if cs is not None:\n",
    "            self.data = []\n",
    "            for inode, ocontexts, itopic, osign in data:\n",
    "                if rd.random() > cs[iword]:\n",
    "                    self.data.append((inode, ocontexts, itopic, osign))\n",
    "        else:\n",
    "            self.data = data\n",
    "        self.idx2context = idx2context\n",
    "        self.negative_data = []\n",
    "        self.positive_data = []\n",
    "        self.us = us\n",
    "        if self.us:\n",
    "            print('spliting negative and positive contexts...')\n",
    "            for inode, ocontexts, itopic in tqdm(self.data):\n",
    "                sign = 1 \n",
    "                max_count = len(ocontexts)\n",
    "                count = 0 \n",
    "                while sign == 1 and count < max_count:\n",
    "                    if self.idx2context[ocontexts[count]][-1] == '-':\n",
    "                        sign = 0\n",
    "                    count += 1\n",
    "                if sign == 0:\n",
    "                    self.negative_data.append((inode, ocontexts, itopic))\n",
    "                else:\n",
    "                    self.positive_data.append((inode, ocontexts, itopic))\n",
    "            print('split negative and positive contexts.')\n",
    "            self.ratio = ratio\n",
    "            self.num_negative = len(self.negative_data)\n",
    "            self.num_positive = len(self.positive_data)\n",
    "            self.min_class = 'negative' if self.num_negative < self.num_positive else 'positive'\n",
    "            self.ratio_dict = {0: self.num_negative, 1: self.num_positive}\n",
    "            print('#negative contexts:', self.num_negative)\n",
    "            print('#positive contexts:', self.num_positive)\n",
    "        \n",
    "    \n",
    "    def undersample(self):\n",
    "        if self.us:\n",
    "            if self.min_class == 'negative':\n",
    "                self.data_undersampled = self.negative_data\n",
    "                self.data_undersampled += rd.sample(self.positive_data, int(self.ratio * self.num_negative))\n",
    "            else:\n",
    "                self.data_undersampled = self.positive_data\n",
    "                self.data_undersampled += rd.sample(self.negative_data, int(self.ratio * self.num_positive))\n",
    "        else:\n",
    "            self.data_undersampled = self.data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_undersampled)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inode, ocontexts, itopic = self.data_undersampled[idx]\n",
    "        return inode, np.array(ocontexts), itopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca276a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_samples, idx2context, idx2node, idx2topic, cc, us=False, ratio=0.8,\n",
    "          ss_t=1e-5, wgts=False, e_dim=64, n_negs=20, epoch=100, mb=4096, op='addition'):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        cc dict: context count\n",
    "        ss_t float: subsampling threshold\n",
    "        wgts np.array: context weights\n",
    "        mb int: minibatch size\n",
    "    Returns:\n",
    "        idx2vec np.array: trained node embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    global model \n",
    "    \n",
    "    cf = np.array([cc[context] for context in idx2context])\n",
    "    cf = cf / cf.sum()\n",
    "    cs = 1 - np.sqrt(ss_t / cf)\n",
    "    cs = np.clip(cs, 0, 1)\n",
    "    context_vocab_size = len(idx2context)\n",
    "    node_vocab_size = len(idx2node)\n",
    "    topic_vocab_size = len(idx2topic)\n",
    "    weights = cf if wgts else None\n",
    "    print('Initialize model...')\n",
    "    model = Embeddings(node_vocab_size=node_vocab_size,\n",
    "                       topic_vocab_size=topic_vocab_size, \n",
    "                       context_vocab_size=context_vocab_size, \n",
    "                       embedding_size=e_dim, \n",
    "                       op=op)\n",
    "    if t.cuda.device_count() > 1:\n",
    "        print(\"Used:\", t.cuda.device_count(), \"GPUs\")\n",
    "        model = nn.DataParallel(model, device_ids=[0, 1, 6, 7], output_device=6)\n",
    "        sgns = SGNS(embedding=model.module, \n",
    "                    n_negs=n_negs, \n",
    "                    weights=weights).to(device)\n",
    "    else:\n",
    "        sgns = SGNS(embedding=model, \n",
    "                    n_negs=n_negs, \n",
    "                    weights=weights).to(device)\n",
    "    print('Initialized model...')\n",
    "    optim = Adam(sgns.parameters())\n",
    "    dataset = PermutedSubsampledCorpus(training_samples, idx2context, us, ratio)\n",
    "    print('Input data ready.')\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        dataset.undersample()\n",
    "        dataloading = DataLoader(dataset, batch_size=mb, shuffle=True)\n",
    "        total_batches = int(np.ceil(len(dataset) / mb))\n",
    "        pbar = tqdm(dataloading)\n",
    "        pbar.set_description(\"[Epoch {}]\".format(epoch))\n",
    "        for inode, ocontexts, itopic in pbar:\n",
    "            loss = sgns(inode, ocontexts, itopic)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    if t.cuda.device_count() > 1:\n",
    "        idx2vec = model.module.ivectors.weight.data.cpu().numpy()\n",
    "        idx2topic = model.module.itopics.weight.data.cpu().numpy()\n",
    "    else:\n",
    "        idx2vec = model.ivectors.weight.data.cpu().numpy()\n",
    "        idx2topic = model.itopics.weight.data.cpu().numpy()\n",
    "    \n",
    "    return idx2vec, idx2topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4991c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "dataset = 'birdwatch'\n",
    "dataset_type = 'hetero'\n",
    "p = 1.5\n",
    "q = .5\n",
    "nw = 5 \n",
    "wl = 40\n",
    "workers = 3 \n",
    "wind = 5\n",
    "wgts = True\n",
    "sst = 1e-5\n",
    "edim = 64 \n",
    "nnegs = 20 \n",
    "ep = 5 \n",
    "mb = 4096\n",
    "op = 'addition'\n",
    "\n",
    "edge_data = pd.read_csv('cached_data/full_datasets/{}.csv'.format(dataset)\n",
    "databuilder = utils.DataBuilder(edge_data, dataset_name=dataset)\n",
    "dataloader = utils.Dataloader(dataset_name=dataset)\n",
    "\n",
    "topic2id = dataloader.topic2idx\n",
    "id2topic = dataloader.idx2topic\n",
    "\n",
    "for _ in range(5):\n",
    "    print('Training set: {}'.format(_+1))\n",
    "    data = DataProcessor(edge_data=dataloader.training_data[dataset_type][_+1], \n",
    "                         node2idx=dataloader.node2idx,\n",
    "                         idx2node=dataloader.idx2node,\n",
    "                         topic2idx=topic2id,\n",
    "                         idx2topic=id2topic,\n",
    "                         dataset=dataloader.dataset_name,\n",
    "                         dataset_type=dataset_type,\n",
    "                         path_walks='cached_data/SE/{data}_type{tp}_walks_train{t}' \\\n",
    "                         .format(data=dataset, tp=dataset_type, t=_+1),\n",
    "                         path_contexts='cached_data/SE/{data}_type{tp}_contexts_train{t}' \\\n",
    "                         .format(data=dataset, tp=dataset_type, t=_+1))\n",
    "    data.build_training_data(p=p, q=q, nw=nw, wl=wl, wind=wind, workers=workers)\n",
    "    idx2vec, idx2topic  = train(training_samples=data.training_samples.data,\n",
    "                                idx2context=data.training_samples.idx2context, \n",
    "                                idx2node=data.training_samples.idx2node, \n",
    "                                idx2topic=data.training_samples.idx2topic,\n",
    "                                cc=data.training_samples.cc,\n",
    "                                us=False,\n",
    "                                ratio=1.,\n",
    "                                ss_t=sst, \n",
    "                                wgts=wgts, \n",
    "                                e_dim=edim, \n",
    "                                n_negs=nnegs, \n",
    "                                epoch=ep, \n",
    "                                mb=mb, \n",
    "                                op=op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
